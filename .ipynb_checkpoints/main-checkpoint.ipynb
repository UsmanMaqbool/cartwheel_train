{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\" NetVLAD training. Using features from deeper VGG layers.\n",
    "#\n",
    "#    - Input -- BN -- MobileNet -- NetVLAD\n",
    "#    - Deeper features (instead of currently using very shallow ones)\n",
    "#    - Verify my pairwise_loss also implement triplet loss\n",
    "#    - Keras sequence use\n",
    "#\n",
    "#        Author  : Manohar Kuse <mpkuse@connect.ust.hk>\n",
    "#        Created : 15th Oct, 2018\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import keras\n",
    "import sys\n",
    "import code\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomNets\n",
    "from CustomNets import NetVLADLayer\n",
    "from CustomNets import dataload_, do_typical_data_aug\n",
    "from CustomNets import make_from_mobilenet, make_from_vgg16\n",
    "\n",
    "# CustomLoses\n",
    "from CustomLosses import triplet_loss2_maker, allpair_hinge_loss_maker, allpair_count_goodfit_maker, positive_set_deviation_maker, allpair_hinge_loss_with_positive_set_deviation_maker\n",
    "\n",
    "from InteractiveLogger import InteractiveLogger\n",
    "\n",
    "\n",
    "# Data\n",
    "from TimeMachineRender import TimeMachineRender\n",
    "# from PandaRender import NetVLADRenderer\n",
    "from WalksRenderer import WalksRenderer\n",
    "from PittsburgRenderer import PittsburgRenderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quit():\n",
    "    sys.exit()\n",
    "\n",
    "# TODO : removal\n",
    "class WSequence(keras.utils.Sequence):\n",
    "    \"\"\"  This class depends on CustomNets.dataload_ for loading data. \"\"\"\n",
    "    def __init__(self, nP, nN, n_samples=(500,-1), initial_epoch=0 ):\n",
    "\n",
    "        assert( type(n_samples) == type(()) )\n",
    "        assert( len(n_samples) == 2 )\n",
    "        self.n_samples_tokyo = n_samples[0]\n",
    "        self.n_samples_pitts = n_samples[1]\n",
    "        self.epoch = initial_epoch\n",
    "        self.batch_size = 4\n",
    "        self.refresh_data_after_n_epochs = 20\n",
    "        # self.n_samples = n_samples\n",
    "\n",
    "\n",
    "        # This will load the data\n",
    "        self.D = dataload_( n_tokyoTimeMachine=self.n_samples_tokyo, n_Pitssburg=self.n_samples_pitts, nP=nP, nN=nN )\n",
    "        print 'dataload_ returned len(self.D)=', len(self.D), 'self.D[0].shape=', self.D[0].shape\n",
    "        self.y = np.zeros( len(self.D) )\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.D) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.D[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        return np.array( batch_x ), np.array( batch_y )\n",
    "       #TODO: Can return another number (sample_weight) for the sample. Which can be judge say by GMS matcher. If we see higher matches amongst +ve set ==> we have good positive samples,\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        N = self.refresh_data_after_n_epochs\n",
    "\n",
    "        if self.epoch % N == 0 and self.epoch > 0 :\n",
    "            print '[on_epoch_end] done %d epochs, so load new data\\t' %(N), int_logr.dir()\n",
    "            # Sample Data\n",
    "            self.D = dataload_( n_tokyoTimeMachine=self.n_samples_tokyo, n_Pitssburg=self.n_samples_pitts, nP=nP, nN=nN )\n",
    "\n",
    "\n",
    "\n",
    "            # if self.epoch > 400:\n",
    "            if self.epoch > 400 and self.n_samples_pitts<0:\n",
    "                # Data Augmentation after 400 epochs. Only do for Tokyo which are used for training. ie. dont augment Pitssburg.\n",
    "                self.D = do_typical_data_aug( self.D )\n",
    "\n",
    "            print 'dataload_ returned len(self.D)=', len(self.D), 'self.D[0].shape=', self.D[0].shape\n",
    "            self.y = np.zeros( len(self.D) )\n",
    "            # modify data\n",
    "        self.epoch += 1\n",
    "\n",
    "\n",
    "# TODO This is a simpler implementation of WSequence. Eventually delete WSequence\n",
    "class PitsSequence(keras.utils.Sequence):\n",
    "    \"\"\"  This class depends on CustomNets.dataload_ for loading data. \"\"\"\n",
    "    def __init__(self, PTS_BASE, nP, nN, n_samples=500, initial_epoch=0 ):\n",
    "\n",
    "        # assert( type(n_samples) == type(()) )\n",
    "        self.n_samples_pitts = int(n_samples)\n",
    "        self.epoch = initial_epoch\n",
    "        self.batch_size = 4\n",
    "        self.refresh_data_after_n_epochs = 20\n",
    "        self.nP = nP\n",
    "        self.nN = nN\n",
    "        # self.n_samples = n_samples\n",
    "        print tcolor.OKGREEN, '-------------PitsSequence Config--------------', tcolor.ENDC\n",
    "        print 'n_samples  : ', self.n_samples_pitts\n",
    "        print 'batch_size : ', self.batch_size\n",
    "        print 'refresh_data_after_n_epochs : ', self.refresh_data_after_n_epochs\n",
    "        print tcolor.OKGREEN, '----------------------------------------------', tcolor.ENDC\n",
    "\n",
    "\n",
    "        # This will load the data\n",
    "        # self.D = dataload_( n_tokyoTimeMachine=self.n_samples_tokyo, n_Pitssburg=self.n_samples_pitts, nP=nP, nN=nN )\n",
    "        # print 'dataload_ returned len(self.D)=', len(self.D), 'self.D[0].shape=', self.D[0].shape\n",
    "        # self.y = np.zeros( len(self.D) )\n",
    "\n",
    "\n",
    "        # PTS_BASE = '/Bulk_Data/data_Akihiko_Torii/Pitssburg/'\n",
    "        self.pr = PittsburgRenderer( PTS_BASE )\n",
    "        self.D = self.pr.step_n_times(n_samples=500, nP=nP, nN=nN, resize=(320,240), return_gray=True, ENABLE_IMSHOW=True )\n",
    "        print 'len(D)=', len(self.D), '\\tD[0].shape=', self.D[0].shape\n",
    "        self.y = np.zeros( len(self.D) )\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.D) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.D[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        # return np.array( batch_x ), np.array( batch_y )\n",
    "        return np.array( batch_x )*1./255. - 0.5, np.array( batch_y )\n",
    "       #TODO: Can return another number (sample_weight) for the sample. Which can be judge say by GMS matcher. If we see higher matches amongst +ve set ==> we have good positive samples,\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        N = self.refresh_data_after_n_epochs\n",
    "\n",
    "        if self.epoch % N == 0 and self.epoch > 0 :\n",
    "            print '[on_epoch_end] done %d epochs, so load new data\\t' %(N), int_logr.dir()\n",
    "            # Sample Data\n",
    "            # self.D = dataload_( n_tokyoTimeMachine=self.n_samples_tokyo, n_Pitssburg=self.n_samples_pitts, nP=nP, nN=nN )\n",
    "\n",
    "            self.D = self.pr.step_n_times(n_samples=500, nP=self.nP, nN=self.nN, resize=(320,240), return_gray=True, ENABLE_IMSHOW=True )\n",
    "            print 'len(D)=', len(self.D), '\\tD[0].shape=', self.D[0].shape\n",
    "\n",
    "\n",
    "            # if self.epoch > 400:\n",
    "            if self.epoch > 400 and self.n_samples_pitts<0:\n",
    "                # Data Augmentation after 400 epochs. Only do for Tokyo which are used for training. ie. dont augment Pitssburg.\n",
    "                self.D = do_typical_data_aug( self.D )\n",
    "\n",
    "            print 'dataload_ returned len(self.D)=', len(self.D), 'self.D[0].shape=', self.D[0].shape\n",
    "            self.y = np.zeros( len(self.D) )\n",
    "            # modify data\n",
    "        self.epoch += 1\n",
    "\n",
    "\n",
    "\n",
    "class CustomModelCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, model_tosave, int_logr ):\n",
    "        self.m_model = model_tosave\n",
    "        self.m_int_logr = int_logr\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if epoch>0 and epoch%200 == 0:\n",
    "            fname = self.m_int_logr.dir() + '/core_model.%d.keras' %(epoch)\n",
    "            print 'Save Intermediate Model : ', fname\n",
    "            self.m_model.save( fname )\n",
    "\n",
    "        if epoch%5 == 0:\n",
    "            print 'm_int_logr=', self.m_int_logr.dir()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "Unable to open file (unable to open file: name = '/app/datasets/models.keras/vgg16_new/block5_pool_k16_allpairloss/core_model.600.keras', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bb39d0a86ca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minitial_epoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Load Previous Weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0mint_logr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/core_model.%d.keras'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: Unable to open file (unable to open file: name = '/app/datasets/models.keras/vgg16_new/block5_pool_k16_allpairloss/core_model.600.keras', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training\n",
    "if __name__ == '__main__':\n",
    "    from keras.backend.tensorflow_backend import set_session\n",
    "    import tensorflow as tf\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "    # config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tf.Session(config=config))\n",
    "\n",
    "    image_nrows = 240\n",
    "    image_ncols = 320\n",
    "    image_nchnl = 3\n",
    "\n",
    "    nP = 6\n",
    "    nN = 6\n",
    "    # int_logr = InteractiveLogger( './models.keras/mobilenet_conv7_allpairloss/' )\n",
    "    # int_logr = InteractiveLogger( './models.keras/mobilenet_conv7_tripletloss2/' )\n",
    "\n",
    "    # int_logr = InteractiveLogger( './models.keras/mobilenet_conv7_quash_chnls_allpairloss/' )\n",
    "    # int_logr = InteractiveLogger( './models.keras/mobilenet_conv7_quash_chnls_tripletloss2_K64/' )\n",
    "\n",
    "    # int_logr = InteractiveLogger( './models.keras/vgg16/block5_pool_k32_allpairloss' )\n",
    "    # int_logr = InteractiveLogger( './models.keras/vgg16/block5_pool_k32_tripletloss2' )\n",
    "\n",
    "    # int_logr = InteractiveLogger( './models.keras/mobilenet_new/pw13_quash_chnls_k16_allpairloss' )\n",
    "    # int_logr = InteractiveLogger( './models.keras/mobilenet_new/pw13_quash_chnls_k16_tripletloss2' )\n",
    "\n",
    "    int_logr = InteractiveLogger( '/app/datasets/models.keras/tmp_staticnormalized_images/' )\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Core Model Setup\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Build\n",
    "    input_img = keras.layers.Input( shape=(image_nrows, image_ncols, image_nchnl ) )\n",
    "    # cnn = make_from_mobilenet( input_img, layer_name='conv_pw_13_relu', kernel_regularizer=None )\n",
    "    cnn = make_from_vgg16( input_img, layer_name='block5_pool' )\n",
    "    # Reduce nChannels of the output.\n",
    "    # @ Downsample (Optional)\n",
    "    if False: #Downsample last layer (Reduce nChannels of the output.)\n",
    "        cnn_dwn = keras.layers.Conv2D( 256, (1,1), padding='same', activation='relu' )( cnn )\n",
    "        cnn_dwn = keras.layers.normalization.BatchNormalization()( cnn_dwn )\n",
    "        cnn_dwn = keras.layers.Conv2D( 64, (1,1), padding='same', activation='relu' )( cnn_dwn )\n",
    "        cnn_dwn = keras.layers.normalization.BatchNormalization()( cnn_dwn )\n",
    "        cnn = cnn_dwn\n",
    "\n",
    "    out, out_amap = NetVLADLayer(num_clusters = 32)( cnn )\n",
    "    model = keras.models.Model( inputs=input_img, outputs=out )\n",
    "\n",
    "    # Plot\n",
    "    model.summary()\n",
    "    keras.utils.plot_model( model, to_file=int_logr.dir()+'/core.png', show_shapes=True )\n",
    "    int_logr.add_file( 'model.json', model.to_json() )\n",
    "\n",
    "\n",
    "    initial_epoch = 0\n",
    "    if initial_epoch > 0:\n",
    "        # Load Previous Weights\n",
    "        model.load_weights(  int_logr.dir() + '/core_model.%d.keras' %(initial_epoch) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # TimeDistributed\n",
    "    #--------------------------------------------------------------------------\n",
    "    t_input = keras.layers.Input( shape=(1+nP+nN, image_nrows, image_ncols, image_nchnl ) )\n",
    "    t_out = keras.layers.TimeDistributed( model )( t_input )\n",
    "\n",
    "    t_model = keras.models.Model( inputs=t_input, outputs=t_out )\n",
    "\n",
    "    t_model.summary()\n",
    "    keras.utils.plot_model( t_model, to_file=int_logr.dir()+'/core_t.png', show_shapes=True )\n",
    "    print 'Write Directory : ', int_logr.dir()\n",
    "    # int_logr.fire_editor()\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Compile\n",
    "    #--------------------------------------------------------------------------\n",
    "    sgdopt = keras.optimizers.Adadelta( )\n",
    "    # sgdopt = keras.optimizers.Adam(  )\n",
    "\n",
    "    loss = triplet_loss2_maker(nP=nP, nN=nN, epsilon=0.3)\n",
    "    # loss = allpair_hinge_loss_with_positive_set_deviation_maker(nP=nP, nN=nN, epsilon=0.3, opt_lambda=0.5 )\n",
    "    # loss = allpair_hinge_loss_maker( nP=nP, nN=nN, epsilon=0.3 )\n",
    "\n",
    "    metrics = [ allpair_count_goodfit_maker( nP=nP, nN=nN, epsilon=0.3 ),\n",
    "                positive_set_deviation_maker(nP=nP, nN=nN)\n",
    "              ]\n",
    "\n",
    "    # t_model.compile( loss=allpair_hinge_loss, optimizer=sgdopt, metrics=[allpair_count_goodfit] )\n",
    "    t_model.compile( loss=loss, optimizer=sgdopt, metrics=metrics )\n",
    "\n",
    "\n",
    "    import tensorflow as tf\n",
    "    tb = tf.keras.callbacks.TensorBoard( log_dir=int_logr.dir() )\n",
    "    saver_cb = CustomModelCallback( model, int_logr )\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.6, patience=75, verbose=1, min_lr=0.1)\n",
    "\n",
    "\n",
    "    history = t_model.fit_generator( generator=PitsSequence('/app/datasets/NetvLad/Pittsburgh/' ,nP=nP, nN=nN, n_samples=500, initial_epoch=initial_epoch),\n",
    "                            epochs=2200, verbose=1, initial_epoch=initial_epoch,\n",
    "                            validation_data = PitsSequence('/app/datasets/NetvLad/Pitssburg_validation/', nP=nP, nN=nN, n_samples=500 ),\n",
    "                            callbacks=[tb,saver_cb,reduce_lr]\n",
    "                         )\n",
    "    print 'Save Final Model : ',  int_logr.dir() + '/core_model.keras'\n",
    "    model.save( int_logr.dir() + '/core_model.keras' )\n",
    "\n",
    "    # print 'Save Json : ', int_logr.dir()+'/history.json'\n",
    "    # with open( int_logr.dir()+'/history.json', 'w') as f:\n",
    "    #     json.dump(history.history, f)\n",
    "    #\n",
    "    #\n",
    "    # print 'Save History : ', int_logr.dir()+'/history.pickle'\n",
    "    # with open( int_logr.dir()+'/history.pickle', 'wb' ) as handle:\n",
    "    #     pickle.dump(history, handle )    initial_epoch = 600\n",
    "    if initial_epoch > 0:\n",
    "        # Load Previous Weights\n",
    "        model.load_weights(  int_logr.dir() + '/core_model.%d.keras' %(initial_epoch) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
