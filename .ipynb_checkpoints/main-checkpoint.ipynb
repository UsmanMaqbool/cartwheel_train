{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\" NetVLAD training. Using features from deeper VGG layers.\n",
    "#\n",
    "#    - Input -- BN -- MobileNet -- NetVLAD\n",
    "#    - Deeper features (instead of currently using very shallow ones)\n",
    "#    - Verify my pairwise_loss also implement triplet loss\n",
    "#    - Keras sequence use\n",
    "#\n",
    "#        Author  : Manohar Kuse <mpkuse@connect.ust.hk>\n",
    "#        Created : 15th Oct, 2018\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import keras\n",
    "\n",
    "import code\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomNets\n",
    "from CustomNets import NetVLADLayer\n",
    "from CustomNets import dataload_, do_typical_data_aug\n",
    "from CustomNets import make_from_mobilenet, make_from_vgg16\n",
    "\n",
    "# CustomLoses\n",
    "from CustomLosses import triplet_loss2_maker, allpair_hinge_loss_maker, allpair_count_goodfit_maker, positive_set_deviation_maker, allpair_hinge_loss_with_positive_set_deviation_maker\n",
    "\n",
    "from InteractiveLogger import InteractiveLogger\n",
    "\n",
    "\n",
    "# Data\n",
    "from TimeMachineRender import TimeMachineRender\n",
    "# from PandaRender import NetVLADRenderer\n",
    "from WalksRenderer import WalksRenderer\n",
    "from PittsburgRenderer import PittsburgRenderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSequence(keras.utils.Sequence):\n",
    "    \"\"\"  This class depends on CustomNets.dataload_ for loading data. \"\"\"\n",
    "    def __init__(self, nP, nN, n_samples=(500,-1), initial_epoch=0 ):\n",
    "\n",
    "        assert( type(n_samples) == type(()) )\n",
    "        assert( len(n_samples) == 2 )\n",
    "        self.n_samples_tokyo = n_samples[0]\n",
    "        self.n_samples_pitts = n_samples[1]\n",
    "        self.epoch = initial_epoch\n",
    "        self.batch_size = 4\n",
    "        self.refresh_data_after_n_epochs = 20\n",
    "        # self.n_samples = n_samples\n",
    "\n",
    "\n",
    "\n",
    "        self.D = dataload_( n_tokyoTimeMachine=self.n_samples_tokyo, n_Pitssburg=self.n_samples_pitts, nP=nP, nN=nN )\n",
    "        print 'dataload_ returned len(self.D)=', len(self.D), 'self.D[0].shape=', self.D[0].shape\n",
    "        self.y = np.zeros( len(self.D) )\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.D) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.D[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        return np.array( batch_x ), np.array( batch_y )\n",
    "       #TODO: Can return another number (sample_weight) for the sample. Which can be judge say by GMS matcher. If we see higher matches amongst +ve set ==> we have good positive samples,\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        N = self.refresh_data_after_n_epochs\n",
    "\n",
    "        if self.epoch % N == 0 and self.epoch > 0 :\n",
    "            print '[on_epoch_end] done %d epochs, so load new data\\t' %(N), int_logr.dir()\n",
    "            # Sample Data\n",
    "            self.D = dataload_( n_tokyoTimeMachine=self.n_samples_tokyo, n_Pitssburg=self.n_samples_pitts, nP=nP, nN=nN )\n",
    "\n",
    "\n",
    "\n",
    "            # if self.epoch > 400:\n",
    "            if self.epoch > 400 and self.n_samples_pitts<0:\n",
    "                # Data Augmentation after 400 epochs. Only do for Tokyo which are used for training. ie. dont augment Pitssburg.\n",
    "                self.D = do_typical_data_aug( self.D )\n",
    "\n",
    "            print 'dataload_ returned len(self.D)=', len(self.D), 'self.D[0].shape=', self.D[0].shape\n",
    "            self.y = np.zeros( len(self.D) )\n",
    "            # modify data\n",
    "        self.epoch += 1\n",
    "\n",
    "\n",
    "class CustomModelCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, model_tosave, int_logr ):\n",
    "        self.m_model = model_tosave\n",
    "        self.m_int_logr = int_logr\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if epoch>0 and epoch%200 == 0:\n",
    "            fname = self.m_int_logr.dir() + '/core_model.%d.keras' %(epoch)\n",
    "            print 'Save Intermediate Model : ', fname\n",
    "            self.m_model.save( fname )\n",
    "\n",
    "        if epoch%20 == 0:\n",
    "            print 'm_int_logr=', self.m_int_logr.dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "if __name__ == '__main__':\n",
    "    from keras.backend.tensorflow_backend import set_session\n",
    "    import tensorflow as tf\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "    # config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tf.Session(config=config))\n",
    "\n",
    "    image_nrows = 240\n",
    "    image_ncols = 320\n",
    "    image_nchnl = 3\n",
    "\n",
    "    nP = 6\n",
    "    nN = 6\n",
    "    # int_logr = InteractiveLogger( './models.keras/mobilenet_conv7_allpairloss/' )\n",
    "    # int_logr = InteractiveLogger( './models.keras/mobilenet_conv7_tripletloss2/' )\n",
    "\n",
    "    # int_logr = InteractiveLogger( './models.keras/mobilenet_conv7_quash_chnls_allpairloss/' )\n",
    "    # int_logr = InteractiveLogger( './models.keras/mobilenet_conv7_quash_chnls_tripletloss2_K64/' )\n",
    "\n",
    "    # int_logr = InteractiveLogger( './models.keras/vgg16/block5_pool_k48_tripletloss2' )\n",
    "    int_logr = InteractiveLogger( './models.keras/vgg16_new/block5_pool_k16_allpairloss' )\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Core Model Setup\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Build\n",
    "    input_img = keras.layers.Input( shape=(image_nrows, image_ncols, image_nchnl ) )\n",
    "    # cnn = make_from_mobilenet( input_img )\n",
    "    cnn = make_from_vgg16( input_img, layer_name='block5_pool' )\n",
    "    # Reduce nChannels of the output.\n",
    "    # @ Downsample (Optional)\n",
    "    if False: #Downsample last layer (Reduce nChannels of the output.)\n",
    "        cnn_dwn = keras.layers.Conv2D( 256, (1,1), padding='same', activation='relu' )( cnn )\n",
    "        cnn_dwn = keras.layers.normalization.BatchNormalization()( cnn_dwn )\n",
    "        cnn_dwn = keras.layers.Conv2D( 32, (1,1), padding='same', activation='relu' )( cnn_dwn )\n",
    "        cnn_dwn = keras.layers.normalization.BatchNormalization()( cnn_dwn )\n",
    "        cnn = cnn_dwn\n",
    "\n",
    "    out, out_amap = NetVLADLayer(num_clusters = 16)( cnn )\n",
    "    model = keras.models.Model( inputs=input_img, outputs=out )\n",
    "\n",
    "    # Plot\n",
    "    model.summary()\n",
    "    keras.utils.plot_model( model, to_file=int_logr.dir()+'/core.png', show_shapes=True )\n",
    "    int_logr.add_file( 'model.json', model.to_json() )\n",
    "\n",
    "\n",
    "    initial_epoch = 600\n",
    "    if initial_epoch > 0:\n",
    "        # Load Previous Weights\n",
    "        model.load_weights(  int_logr.dir() + '/core_model.%d.keras' %(initial_epoch) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #--------------------------------------------------------------------------\n",
    "    # TimeDistributed\n",
    "    #--------------------------------------------------------------------------\n",
    "    t_input = keras.layers.Input( shape=(1+nP+nN, image_nrows, image_ncols, image_nchnl ) )\n",
    "    t_out = keras.layers.TimeDistributed( model )( t_input )\n",
    "\n",
    "    t_model = keras.models.Model( inputs=t_input, outputs=t_out )\n",
    "\n",
    "    t_model.summary()\n",
    "    keras.utils.plot_model( t_model, to_file=int_logr.dir()+'/core_t.png', show_shapes=True )\n",
    "    print 'Write Directory : ', int_logr.dir()\n",
    "    # int_logr.fire_editor()\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Compile\n",
    "    #--------------------------------------------------------------------------\n",
    "    sgdopt = keras.optimizers.Adadelta( )\n",
    "    # sgdopt = keras.optimizers.Adam(  )\n",
    "\n",
    "    # loss = triplet_loss2_maker(nP=nP, nN=nN, epsilon=0.3)\n",
    "    # loss = allpair_hinge_loss_with_positive_set_deviation_maker(nP=nP, nN=nN, epsilon=0.3, opt_lambda=0.5 )\n",
    "    loss = allpair_hinge_loss_maker( nP=nP, nN=nN, epsilon=0.25 )\n",
    "\n",
    "    metrics = [ allpair_count_goodfit_maker( nP=nP, nN=nN, epsilon=0.3 ),\n",
    "                positive_set_deviation_maker(nP=nP, nN=nN)\n",
    "              ]\n",
    "\n",
    "    # t_model.compile( loss=allpair_hinge_loss, optimizer=sgdopt, metrics=[allpair_count_goodfit] )\n",
    "    t_model.compile( loss=loss, optimizer=sgdopt, metrics=metrics )\n",
    "\n",
    "\n",
    "    import tensorflow as tf\n",
    "    tb = tf.keras.callbacks.TensorBoard( log_dir=int_logr.dir() )\n",
    "    saver_cb = CustomModelCallback( model, int_logr )\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=25, verbose=1, min_lr=0.001)\n",
    "\n",
    "\n",
    "    history = t_model.fit_generator( generator=WSequence(nP, nN, n_samples=(500,-1), initial_epoch=initial_epoch),\n",
    "                            epochs=1200, verbose=1, initial_epoch=initial_epoch,\n",
    "                            validation_data = WSequence(nP, nN, n_samples=(-1,500) ),\n",
    "                            callbacks=[tb,saver_cb,reduce_lr]\n",
    "                         )\n",
    "    print 'Save Final Model : ',  int_logr.dir() + '/core_model.keras'\n",
    "    model.save( int_logr.dir() + '/core_model.keras' )\n",
    "\n",
    "    print 'Save Json : ', int_logr.dir()+'/history.json'\n",
    "    with open( int_logr.dir()+'/history.json', 'w') as f:\n",
    "        json.dump(history.history, f)\n",
    "\n",
    "\n",
    "    print 'Save History : ', int_logr.dir()+'/history.pickle'\n",
    "    with open( int_logr.dir()+'/history.pickle', 'wb' ) as handle:\n",
    "        pickle.dump(history, handle )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
